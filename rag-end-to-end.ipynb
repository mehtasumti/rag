{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dd0cd4af-4c15-4a45-bd04-6e3b053bdcdc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Folders created!\n",
      "\n",
      "Current directory: /home/sagemaker-user\n",
      "\n",
      "Folder structure:\n",
      "rag-project/\n",
      "  data/\n",
      "  .ipynb_checkpoints/\n"
     ]
    }
   ],
   "source": [
    "# Create project directories\n",
    "import os\n",
    "\n",
    "# Create main project folder\n",
    "os.makedirs('rag-project/data', exist_ok=True)\n",
    "print(\"âœ… Folders created!\")\n",
    "\n",
    "# Verify\n",
    "print(\"\\nCurrent directory:\", os.getcwd())\n",
    "print(\"\\nFolder structure:\")\n",
    "for root, dirs, files in os.walk('rag-project'):\n",
    "    level = root.replace('rag-project', '').count(os.sep)\n",
    "    indent = ' ' * 2 * level\n",
    "    print(f'{indent}{os.path.basename(root)}/')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0318054a-1df0-4b85-941c-444a183a2934",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3 Î¼s, sys: 1 Î¼s, total: 4 Î¼s\n",
      "Wall time: 5.72 Î¼s\n",
      "\u001b[31mERROR: Could not find a version that satisfies the requirement faiss-cpu==1.7.4 (from versions: 1.8.0, 1.8.0.post1, 1.9.0, 1.9.0.post1, 1.10.0, 1.11.0, 1.11.0.post1, 1.12.0, 1.13.0, 1.13.1, 1.13.2)\u001b[0m\u001b[31m\n",
      "\u001b[0m\u001b[31mERROR: No matching distribution found for faiss-cpu==1.7.4\u001b[0m\u001b[31m\n",
      "\u001b[0mâœ… All packages installed successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: Install all required packages\n",
    "# This might take 3-5 minutes\n",
    "\n",
    "%time\n",
    "!pip install -q langchain==0.1.0\n",
    "!pip install -q langchain-community==0.0.20\n",
    "!pip install -q faiss-cpu==1.7.4\n",
    "!pip install -q pypdf==4.0.0\n",
    "!pip install -q wikipedia==1.4.0\n",
    "!pip install -q youtube-search-python==1.6.6\n",
    "\n",
    "print(\"âœ… All packages installed successfully!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ad06cb6b-1b95-49e6-a915-934529a27a51",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ… Document created: rag-project/data/aws_bedrock_guide.txt\n",
      "ğŸ“Š Document size: 7483 characters\n",
      "ğŸ“„ Approximate words: 1057 words\n",
      "âœ… File verified at: rag-project/data/aws_bedrock_guide.txt\n"
     ]
    }
   ],
   "source": [
    "# Cell 2: Create sample document\n",
    "\n",
    "sample_content = \"\"\"\n",
    "AWS Bedrock: Complete Guide\n",
    "\n",
    "AWS Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) \n",
    "from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon through \n",
    "a single API. It provides the capabilities you need to build generative AI applications with security, \n",
    "privacy, and responsible AI.\n",
    "\n",
    "Key Features of AWS Bedrock:\n",
    "1. Model Choice: Access to multiple foundation models including Claude, Titan, Jurassic, and more\n",
    "2. Customization: Fine-tune models with your own data using techniques like instruction tuning\n",
    "3. Security: Enterprise-grade security with AWS PrivateLink support\n",
    "4. Serverless: No infrastructure to manage, pay only for what you use\n",
    "5. Integration: Seamlessly integrates with other AWS services\n",
    "\n",
    "Retrieval Augmented Generation (RAG):\n",
    "RAG is a powerful technique that enhances language model responses by retrieving relevant information \n",
    "from external knowledge sources. Instead of relying solely on the model's training data, RAG systems:\n",
    "- Retrieve relevant documents from a knowledge base\n",
    "- Augment the prompt with retrieved context\n",
    "- Generate more accurate and contextual responses\n",
    "\n",
    "**How RAG Works**:\n",
    "RAG combines the power of retrieval-based techniques and generative language models, which makes it a highly powerful framework for improving the quality and contextuality of AI-generated content. It operates in a multi-step process:\n",
    "1. **Document Retrieval**:\n",
    "   - RAG systems begin by retrieving relevant documents or data from a knowledge base or external data sources (e.g., databases, document repositories).\n",
    "   - These documents are selected based on their semantic relevance to the user query, typically by using vector-based search methods (e.g., **dense retrieval**, **semantic search**).\n",
    "   \n",
    "2. **Context Augmentation**:\n",
    "   - The retrieved documents or passages are then added to the original query as context, which helps the model generate a more informed and specific response.\n",
    "   - This can be achieved by simply appending the retrieved context to the input prompt of the language model.\n",
    "   \n",
    "3. **Response Generation**:\n",
    "   - The language model, now augmented with the external knowledge, generates a response that is grounded in the context provided, making it more accurate and relevant to the userâ€™s request.\n",
    "   - This step can either use **in-context learning** (where the model adapts based on the augmented context) or traditional fine-tuning for more complex tasks.\n",
    "\n",
    "**Why Use RAG?**\n",
    "1. **Combining Retrieval and Generation**:\n",
    "   - RAG combines the **precision of information retrieval** with the **creativity of generative models**, enabling a more flexible and powerful AI system. \n",
    "   - This allows for a scalable solution where the AI can retrieve accurate information from a large set of documents and generate answers based on the latest available data.\n",
    "   \n",
    "2. **Improving Model Accuracy**:\n",
    "   - By leveraging external knowledge, RAG systems can answer questions that the base language model might not be capable of answering accurately due to its limitations or outdated training data.\n",
    "\n",
    "3. **Filling Knowledge Gaps**:\n",
    "   - RAG can dynamically access up-to-date information, such as recent news or specific domain knowledge, which is often absent from pre-trained models.\n",
    "\n",
    "4. **Reducing Training Costs**:\n",
    "   - Instead of fine-tuning models with massive datasets, RAG can perform well by just leveraging external knowledge sources, which is a more cost-effective and scalable approach.\n",
    "\n",
    "**Applications of RAG**:\n",
    "1. **Customer Support Systems**:\n",
    "   - RAG-powered chatbots can pull relevant company documentation and respond to customer queries by providing personalized answers based on current product data, FAQs, and manuals.\n",
    "   \n",
    "2. **Content Creation**:\n",
    "   - Writers and content creators can use RAG to quickly generate high-quality, relevant articles, blog posts, or marketing copy by retrieving information from multiple sources and generating new content on-demand.\n",
    "   \n",
    "3. **Legal and Medical Document Analysis**:\n",
    "   - RAG can be used to process large legal or medical document sets, providing users with summaries or answers to specific questions based on the document contents.\n",
    "   \n",
    "4. **Code Generation**:\n",
    "   - In software development, RAG can be employed to assist in code generation by retrieving relevant code snippets, documentation, and technical manuals from large codebases or programming guides, allowing developers to generate or troubleshoot code more effectively.\n",
    "\n",
    "**Challenges in Implementing RAG**:\n",
    "1. **Retrieval Precision**:\n",
    "   - The quality of the generated response heavily depends on how well the retrieval system performs. If irrelevant or low-quality documents are retrieved, the final response will also be inaccurate.\n",
    "   \n",
    "2. **Context Length Limitations**:\n",
    "   - Generative models (e.g., GPT-3) have a limited **context window**. This means that if the number of documents retrieved is too large, or if they contain too much content, it might exceed the modelâ€™s input capacity, reducing performance.\n",
    "\n",
    "3. **Knowledge Source Management**:\n",
    "   - Managing and maintaining an up-to-date knowledge base is crucial for RAG systems. If the knowledge base is outdated or incomplete, the model might retrieve outdated information, leading to incorrect or suboptimal answers.\n",
    "\n",
    "4. **Computational Resources**:\n",
    "   - RAG systems often require significant computational resources for both retrieval and generation stages, making them resource-intensive. This can result in higher costs, especially when scaled.\n",
    "\n",
    "Best Practices for RAG Systems:\n",
    "- **Chunk Documents Appropriately** (300-500 tokens): Ensures that each chunk is relevant and digestible for the model.\n",
    "- **Use High-Quality Embeddings** (Titan, OpenAI): High-quality embeddings improve the accuracy and efficiency of both the retrieval and generation steps.\n",
    "- **Implement Proper Error Handling**: Handle cases where no relevant documents are retrieved, or when the model cannot generate a response.\n",
    "- **Cache Frequently Accessed Results**: Cache the documents that are often retrieved, improving performance by reducing the retrieval time.\n",
    "- **Monitor and Log System Performance**: Continuously monitor the performance of both retrieval and generation stages and log results for further improvements.\n",
    "- **Regularly Update Your Knowledge Base**: Ensure the knowledge base remains current, especially in dynamic fields like healthcare, finance, and technology.\n",
    "\n",
    "Vector Embeddings Explained:\n",
    "Vector embeddings are numerical representations of text that capture semantic meaning. They convert \n",
    "words, sentences, or documents into arrays of numbers (vectors) in high-dimensional space. Similar \n",
    "concepts are placed close together in this space, enabling:\n",
    "- Semantic search capabilities\n",
    "- Similarity comparisons\n",
    "- Efficient information retrieval\n",
    "\n",
    "Common Use Cases:\n",
    "1. Intelligent Chatbots: Customer service bots that access company documentation\n",
    "2. Document Analysis: Automated summarization and question-answering over large document sets\n",
    "3. Content Generation: Creating marketing copy, reports, or creative content\n",
    "4. Code Assistance: Generating and explaining code snippets\n",
    "5. Research Tools: Helping researchers find relevant papers and information\n",
    "\n",
    "Security Considerations:\n",
    "- Always use IAM roles with least privilege\n",
    "- Enable encryption at rest and in transit\n",
    "- Implement input validation and sanitization\n",
    "- Use VPC endpoints for private connectivity\n",
    "- Audit and monitor all API calls\n",
    "\"\"\"\n",
    "\n",
    "# Save to file in the data folder\n",
    "file_path = 'rag-project/data/aws_bedrock_guide.txt'\n",
    "\n",
    "with open(file_path, 'w', encoding='utf-8') as f:\n",
    "    f.write(sample_content)\n",
    "\n",
    "print(f\"âœ… Document created: {file_path}\")\n",
    "print(f\"ğŸ“Š Document size: {len(sample_content)} characters\")\n",
    "print(f\"ğŸ“„ Approximate words: {len(sample_content.split())} words\")\n",
    "\n",
    "# Verify file exists\n",
    "import os\n",
    "if os.path.exists(file_path):\n",
    "    print(f\"âœ… File verified at: {file_path}\")\n",
    "else:\n",
    "    print(\"âŒ Error: File not created\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "3fa6eadd-e6d3-42ac-b162-a15b79f3a1a1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“ Files in data folder:\n",
      "--------------------------------------------------\n",
      "  ğŸ“„ aws_bedrock_guide.txt (7,487 bytes)\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: List all files in data folder\n",
    "\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“ Files in data folder:\")\n",
    "print(\"-\" * 50)\n",
    "\n",
    "data_folder = 'rag-project/data'\n",
    "if os.path.exists(data_folder):\n",
    "    files = os.listdir(data_folder)\n",
    "    if files:\n",
    "        for file in files:\n",
    "            file_path = os.path.join(data_folder, file)\n",
    "            size = os.path.getsize(file_path)\n",
    "            print(f\"  ğŸ“„ {file} ({size:,} bytes)\")\n",
    "    else:\n",
    "        print(\"  (empty)\")\n",
    "else:\n",
    "    print(\"  âŒ Data folder not found\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "d2e03267-9676-45de-8453-c50131348cbe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”§ Setting up AWS Bedrock...\n",
      "âœ… Bedrock clients initialized!\n",
      "âœ… Successfully connected to Bedrock!\n",
      "ğŸ“Š Available models: 124\n",
      "\n",
      "ğŸ¯ Checking access to required models...\n",
      "  âœ… amazon.titan-embed-text-v1\n",
      "  âœ… anthropic.claude-3-sonnet-20240229-v1:0\n"
     ]
    }
   ],
   "source": [
    "# Cell 4: Import libraries and setup AWS Bedrock\n",
    "\n",
    "import boto3\n",
    "import json\n",
    "import numpy as np\n",
    "from typing import List, Dict\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "print(\"ğŸ”§ Setting up AWS Bedrock...\")\n",
    "\n",
    "# Initialize Bedrock Runtime client\n",
    "# SageMaker automatically uses the instance role - no credentials needed!\n",
    "bedrock_runtime = boto3.client(\n",
    "    service_name='bedrock-runtime',\n",
    "    region_name='us-east-1'  # Change to your region if needed\n",
    ")\n",
    "\n",
    "bedrock_agent_runtime = boto3.client(\n",
    "    service_name='bedrock-agent-runtime',\n",
    "    region_name='us-east-1'\n",
    ")\n",
    "\n",
    "print(\"âœ… Bedrock clients initialized!\")\n",
    "\n",
    "# Test connection\n",
    "try:\n",
    "    bedrock = boto3.client('bedrock', region_name='us-east-1')\n",
    "    response = bedrock.list_foundation_models()\n",
    "    model_count = len(response.get('modelSummaries', []))\n",
    "    print(f\"âœ… Successfully connected to Bedrock!\")\n",
    "    print(f\"ğŸ“Š Available models: {model_count}\")\n",
    "    \n",
    "    # Show enabled models\n",
    "    print(\"\\nğŸ¯ Checking access to required models...\")\n",
    "    required_models = [\n",
    "        'amazon.titan-embed-text-v1',\n",
    "        'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "    ]\n",
    "    \n",
    "    for model_id in required_models:\n",
    "        try:\n",
    "            # Try to access model info\n",
    "            model_info = bedrock.get_foundation_model(modelIdentifier=model_id)\n",
    "            print(f\"  âœ… {model_id}\")\n",
    "        except:\n",
    "            print(f\"  âŒ {model_id} - NOT ACCESSIBLE\")\n",
    "            print(f\"     Please enable this model in Bedrock Console\")\n",
    "    \n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error connecting to Bedrock: {e}\")\n",
    "    print(\"\\nğŸ” Troubleshooting:\")\n",
    "    print(\"  1. Check if Bedrock models are enabled in AWS Console\")\n",
    "    print(\"  2. Verify IAM role has AmazonBedrockFullAccess policy\")\n",
    "    print(\"  3. Confirm you're in a supported region (us-east-1, us-west-2)\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7bff0cfe-bce3-484d-a1a5-fe47c8a26f43",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ“„ Loading documents...\n",
      "âœ… Loaded 1 document(s)\n",
      "ğŸ“Š Total characters: 7,483\n",
      "\n",
      "ğŸ“ First 200 characters:\n",
      "------------------------------------------------------------\n",
      "\n",
      "AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) \n",
      "from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta,...\n",
      "------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 5: Load documents\n",
    "\n",
    "from langchain.document_loaders import TextLoader\n",
    "import os\n",
    "\n",
    "print(\"ğŸ“„ Loading documents...\")\n",
    "\n",
    "# Define file path\n",
    "file_path = 'rag-project/data/aws_bedrock_guide.txt'\n",
    "\n",
    "# Check if file exists\n",
    "if not os.path.exists(file_path):\n",
    "    print(f\"âŒ File not found: {file_path}\")\n",
    "    print(\"\\nAvailable files:\")\n",
    "    print(os.listdir('rag-project/data'))\n",
    "else:\n",
    "    # Load the document\n",
    "    loader = TextLoader(file_path, encoding='utf-8')\n",
    "    documents = loader.load()\n",
    "    \n",
    "    print(f\"âœ… Loaded {len(documents)} document(s)\")\n",
    "    print(f\"ğŸ“Š Total characters: {sum(len(doc.page_content) for doc in documents):,}\")\n",
    "    print(f\"\\nğŸ“ First 200 characters:\")\n",
    "    print(\"-\" * 60)\n",
    "    print(documents[0].page_content[:200] + \"...\")\n",
    "    print(\"-\" * 60)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "6e3b5867-df73-49e4-ade0-7d0b628a860d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "âœ‚ï¸ Chunking documents...\n",
      "âœ… Created 20 chunks\n",
      "ğŸ“Š Average chunk size: 376 characters\n",
      "\n",
      "ğŸ“ Sample chunks:\n",
      "============================================================\n",
      "\n",
      "Chunk 1 (366 chars):\n",
      "------------------------------------------------------------\n",
      "AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) \n",
      "from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon through \n",
      "a single API. It provides the capabilities you need to build generative AI applications with security, \n",
      "privacy, and responsible AI.\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 2 (421 chars):\n",
      "------------------------------------------------------------\n",
      "Key Features of AWS Bedrock:\n",
      "1. Model Choice: Access to multiple foundation models including Claude, Titan, Jurassic, and more\n",
      "2. Customization: Fine-tune models with your own data using techniques like instruction tuning\n",
      "3. Security: Enterprise-grade security with AWS PrivateLink support\n",
      "4. Serverless: No infrastructure to manage, pay only for what you use\n",
      "5. Integration: Seamlessly integrates with other AWS services\n",
      "------------------------------------------------------------\n",
      "\n",
      "Chunk 3 (388 chars):\n",
      "------------------------------------------------------------\n",
      "Retrieval Augmented Generation (RAG):\n",
      "RAG is a powerful technique that enhances language model responses by retrieving relevant information \n",
      "from external knowledge sources. Instead of relying solely on the model's training data, RAG systems:\n",
      "- Retrieve relevant documents from a knowledge base\n",
      "- Augment the prompt with retrieved context\n",
      "- Generate more accurate and contextual responses\n",
      "------------------------------------------------------------\n",
      "\n",
      "âœ… Stored 20 chunks for embedding\n"
     ]
    }
   ],
   "source": [
    "# Cell 6: Chunk documents into smaller pieces\n",
    "\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
    "\n",
    "print(\"âœ‚ï¸ Chunking documents...\")\n",
    "\n",
    "# Create text splitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    chunk_size=500,           # Maximum characters per chunk\n",
    "    chunk_overlap=50,         # Overlap to maintain context\n",
    "    length_function=len,\n",
    "    separators=[\"\\n\\n\", \"\\n\", \". \", \" \", \"\"]  # Split priority\n",
    ")\n",
    "\n",
    "# Split documents\n",
    "chunks = text_splitter.split_documents(documents)\n",
    "\n",
    "print(f\"âœ… Created {len(chunks)} chunks\")\n",
    "print(f\"ğŸ“Š Average chunk size: {sum(len(c.page_content) for c in chunks) // len(chunks)} characters\")\n",
    "\n",
    "# Show sample chunks\n",
    "print(f\"\\nğŸ“ Sample chunks:\")\n",
    "print(\"=\" * 60)\n",
    "for i, chunk in enumerate(chunks[:3], 1):\n",
    "    print(f\"\\nChunk {i} ({len(chunk.page_content)} chars):\")\n",
    "    print(\"-\" * 60)\n",
    "    print(chunk.page_content)\n",
    "    print(\"-\" * 60)\n",
    "\n",
    "# Store chunk texts for later use\n",
    "chunk_texts = [chunk.page_content for chunk in chunks]\n",
    "print(f\"\\nâœ… Stored {len(chunk_texts)} chunks for embedding\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "78e89241-72bb-4ebb-a785-147dc7a55b93",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ”¢ Generating embeddings with Bedrock Titan...\n",
      "â³ This may take 30-60 seconds...\n",
      "\n",
      "  Processing 5/20 chunks... (25%)\n",
      "  Processing 10/20 chunks... (50%)\n",
      "  Processing 15/20 chunks... (75%)\n",
      "  Processing 20/20 chunks... (100%)\n",
      "\n",
      "âœ… Successfully generated 20 embeddings\n",
      "ğŸ“ Embedding dimension: 1536\n",
      "ğŸ’¾ Total embedding data: 120.00 KB\n"
     ]
    }
   ],
   "source": [
    "# Cell 7: Generate vector embeddings using Bedrock Titan\n",
    "\n",
    "print(\"ğŸ”¢ Generating embeddings with Bedrock Titan...\")\n",
    "print(\"â³ This may take 30-60 seconds...\\n\")\n",
    "\n",
    "def get_embedding(text: str) -> List[float]:\n",
    "    \"\"\"Generate embedding for text using Bedrock Titan\"\"\"\n",
    "    try:\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId='amazon.titan-embed-text-v1',\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=json.dumps({\"inputText\": text})\n",
    "        )\n",
    "        \n",
    "        response_body = json.loads(response['body'].read())\n",
    "        return response_body['embedding']\n",
    "    \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error generating embedding: {e}\")\n",
    "        return None\n",
    "\n",
    "# Generate embeddings for all chunks\n",
    "embeddings = []\n",
    "total_chunks = len(chunk_texts)\n",
    "\n",
    "for i, text in enumerate(chunk_texts, 1):\n",
    "    # Progress indicator\n",
    "    if i % 5 == 0 or i == total_chunks:\n",
    "        print(f\"  Processing {i}/{total_chunks} chunks... ({i*100//total_chunks}%)\")\n",
    "    \n",
    "    embedding = get_embedding(text)\n",
    "    if embedding:\n",
    "        embeddings.append(embedding)\n",
    "    else:\n",
    "        print(f\"  âš ï¸ Failed to generate embedding for chunk {i}\")\n",
    "\n",
    "print(f\"\\nâœ… Successfully generated {len(embeddings)} embeddings\")\n",
    "print(f\"ğŸ“ Embedding dimension: {len(embeddings[0])}\")\n",
    "print(f\"ğŸ’¾ Total embedding data: {len(embeddings) * len(embeddings[0]) * 4 / 1024:.2f} KB\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "96dca8c1-1741-4671-9e08-72079b235a34",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ—„ï¸ Creating FAISS vector store...\n",
      "\n",
      "ğŸ“Š Embedding array shape: (20, 1536)\n",
      "   - Number of vectors: 20\n",
      "   - Vector dimension: 1536\n",
      "\n",
      "âœ… Vector store created successfully!\n",
      "   - Index type: L2 (Euclidean distance)\n",
      "   - Total vectors indexed: 20\n",
      "ğŸ’¾ Index saved to: rag-project/faiss_index.bin\n",
      "âœ… Verified: Loaded index has 20 vectors\n"
     ]
    }
   ],
   "source": [
    "# Cell 8: Create FAISS vector database\n",
    "\n",
    "import faiss\n",
    "import numpy as np\n",
    "\n",
    "print(\"ğŸ—„ï¸ Creating FAISS vector store...\\n\")\n",
    "\n",
    "# Convert embeddings to numpy array\n",
    "embeddings_array = np.array(embeddings).astype('float32')\n",
    "\n",
    "print(f\"ğŸ“Š Embedding array shape: {embeddings_array.shape}\")\n",
    "print(f\"   - Number of vectors: {embeddings_array.shape[0]}\")\n",
    "print(f\"   - Vector dimension: {embeddings_array.shape[1]}\")\n",
    "\n",
    "# Get dimension\n",
    "dimension = embeddings_array.shape[1]\n",
    "\n",
    "# Create FAISS index (L2 distance = Euclidean distance)\n",
    "index = faiss.IndexFlatL2(dimension)\n",
    "\n",
    "# Add vectors to index\n",
    "index.add(embeddings_array)\n",
    "\n",
    "print(f\"\\nâœ… Vector store created successfully!\")\n",
    "print(f\"   - Index type: L2 (Euclidean distance)\")\n",
    "print(f\"   - Total vectors indexed: {index.ntotal}\")\n",
    "\n",
    "# Create mapping from index position to chunk text\n",
    "doc_store = {i: chunk_texts[i] for i in range(len(chunk_texts))}\n",
    "\n",
    "# Optional: Save index to disk\n",
    "index_path = 'rag-project/faiss_index.bin'\n",
    "faiss.write_index(index, index_path)\n",
    "print(f\"ğŸ’¾ Index saved to: {index_path}\")\n",
    "\n",
    "# Verify\n",
    "loaded_index = faiss.read_index(index_path)\n",
    "print(f\"âœ… Verified: Loaded index has {loaded_index.ntotal} vectors\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "aa94d40d-6381-48b0-999d-c0d3a44dcf3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "================================================================================\n",
      "ğŸ§ª TESTING VECTOR SEARCH\n",
      "================================================================================\n",
      "\n",
      "ğŸ” Searching for: 'What is AWS Bedrock?'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0080, Distance: 123.4524)\n",
      "   Text preview: Key Features of AWS Bedrock:\n",
      "1. Model Choice: Access to multiple foundation models including Claude,...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0080, Distance: 124.5595)\n",
      "   Text preview: AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-per...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ” Searching for: 'Explain RAG'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0062, Distance: 159.3218)\n",
      "   Text preview: **Why Use RAG?**\n",
      "1. **Combining Retrieval and Generation**:\n",
      "   - RAG combines the **precision of inf...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0062, Distance: 160.8486)\n",
      "   Text preview: **How RAG Works**:\n",
      "RAG combines the power of retrieval-based techniques and generative language mode...\n",
      "\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ” Searching for: 'How do vector embeddings work?'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0069, Distance: 144.0179)\n",
      "   Text preview: Vector Embeddings Explained:\n",
      "Vector embeddings are numerical representations of text that capture se...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0029, Distance: 344.2964)\n",
      "   Text preview: - These documents are selected based on their semantic relevance to the user query, typically by usi...\n",
      "\n",
      "--------------------------------------------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "# Cell 9: Vector similarity search function\n",
    "\n",
    "def search_similar_chunks(query: str, k: int = 3) -> List[Dict]:\n",
    "    \"\"\"\n",
    "    Search for similar chunks using vector similarity\n",
    "    \n",
    "    Args:\n",
    "        query: Search query text\n",
    "        k: Number of top results to return\n",
    "    \n",
    "    Returns:\n",
    "        List of dictionaries with chunk text and scores\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ” Searching for: '{query}'\")\n",
    "    print(f\"ğŸ“Š Retrieving top {k} results...\\n\")\n",
    "    \n",
    "    # Generate embedding for query\n",
    "    query_embedding = get_embedding(query)\n",
    "    \n",
    "    if query_embedding is None:\n",
    "        print(\"âŒ Failed to generate query embedding\")\n",
    "        return []\n",
    "    \n",
    "    # Convert to numpy array\n",
    "    query_vector = np.array([query_embedding]).astype('float32')\n",
    "    \n",
    "    # Search in FAISS index\n",
    "    distances, indices = index.search(query_vector, k)\n",
    "    \n",
    "    # Prepare results\n",
    "    results = []\n",
    "    for i, (dist, idx) in enumerate(zip(distances[0], indices[0]), 1):\n",
    "        similarity_score = 1 / (1 + float(dist))  # Convert distance to similarity\n",
    "        \n",
    "        results.append({\n",
    "            'rank': i,\n",
    "            'index': int(idx),\n",
    "            'text': doc_store[idx],\n",
    "            'distance': float(dist),\n",
    "            'similarity_score': similarity_score\n",
    "        })\n",
    "        \n",
    "        print(f\"ğŸ“Œ Result {i} (Score: {similarity_score:.4f}, Distance: {dist:.4f})\")\n",
    "        print(f\"   Text preview: {doc_store[idx][:100]}...\")\n",
    "        print()\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Test the search function\n",
    "print(\"=\" * 80)\n",
    "print(\"ğŸ§ª TESTING VECTOR SEARCH\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "test_queries = [\n",
    "    \"What is AWS Bedrock?\",\n",
    "    \"Explain RAG\",\n",
    "    \"How do vector embeddings work?\"\n",
    "]\n",
    "\n",
    "for query in test_queries:\n",
    "    results = search_similar_chunks(query, k=2)\n",
    "    print(\"-\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "fa8d21c5-43c6-4db1-9bed-cb67b89b3b9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ§ª TESTING COMPLETE RAG PIPELINE\n",
      "\n",
      "\n",
      "================================================================================\n",
      "â“ QUESTION: What is Retrieval Augmented Generation and how does it work?\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Step 1: Retrieving relevant context...\n",
      "\n",
      "ğŸ” Searching for: 'What is Retrieval Augmented Generation and how does it work?'\n",
      "ğŸ“Š Retrieving top 3 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0065, Distance: 153.0849)\n",
      "   Text preview: Retrieval Augmented Generation (RAG):\n",
      "RAG is a powerful technique that enhances language model respo...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0060, Distance: 165.7649)\n",
      "   Text preview: **How RAG Works**:\n",
      "RAG combines the power of retrieval-based techniques and generative language mode...\n",
      "\n",
      "ğŸ“Œ Result 3 (Score: 0.0058, Distance: 171.3724)\n",
      "   Text preview: **Why Use RAG?**\n",
      "1. **Combining Retrieval and Generation**:\n",
      "   - RAG combines the **precision of inf...\n",
      "\n",
      "\n",
      "ğŸ¤– Step 2: Generating answer with Claude...\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¬ ANSWER:\n",
      "================================================================================\n",
      "Retrieval Augmented Generation (RAG) is a technique that enhances language model responses by retrieving relevant information from external knowledge sources. It works as follows:\n",
      "\n",
      "â€¢ RAG systems first retrieve relevant documents or data from a knowledge base or external data sources (document retrieval step).\n",
      "\n",
      "â€¢ The retrieved context is then used to augment the input prompt.\n",
      "\n",
      "â€¢ Finally, a language model generates responses based on both the original prompt and the retrieved context.\n",
      "\n",
      "RAG combines the strengths of retrieval-based techniques and generative language models, allowing for more accurate and contextual responses. The key benefits of using RAG include:\n",
      "\n",
      "â€¢ Combining the precision of information retrieval with the creativity of generative models.\n",
      "â€¢ Improving model accuracy by providing up-to-date and relevant information from external sources.\n",
      "â€¢ Enabling a scalable solution where the AI can retrieve information from large knowledge bases.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– SOURCES USED:\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ Source 1 (Similarity: 0.0065)\n",
      "   Retrieval Augmented Generation (RAG):\n",
      "RAG is a powerful technique that enhances language model responses by retrieving relevant information \n",
      "from exte...\n",
      "\n",
      "ğŸ“Œ Source 2 (Similarity: 0.0060)\n",
      "   **How RAG Works**:\n",
      "RAG combines the power of retrieval-based techniques and generative language models, which makes it a highly powerful framework for...\n",
      "\n",
      "ğŸ“Œ Source 3 (Similarity: 0.0058)\n",
      "   **Why Use RAG?**\n",
      "1. **Combining Retrieval and Generation**:\n",
      "   - RAG combines the **precision of information retrieval** with the **creativity of gene...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 10: Complete RAG pipeline with Claude\n",
    "\n",
    "def generate_answer(question: str, context_chunks: List[str]) -> str:\n",
    "    \"\"\"\n",
    "    Generate answer using Claude with retrieved context\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        context_chunks: Retrieved relevant text chunks\n",
    "    \n",
    "    Returns:\n",
    "        Generated answer from Claude\n",
    "    \"\"\"\n",
    "    # Combine context\n",
    "    context = \"\\n\\n\".join(context_chunks)\n",
    "    \n",
    "    # Build prompt with context\n",
    "    prompt = f\"\"\"You are a helpful AI assistant. Use the context below to answer the question accurately.\n",
    "\n",
    "Context:\n",
    "{context}\n",
    "\n",
    "Question: {question}\n",
    "\n",
    "Instructions:\n",
    "- Answer based primarily on the provided context\n",
    "- If the context doesn't contain enough information, say so\n",
    "- Be concise but thorough\n",
    "- Use bullet points for lists when appropriate\n",
    "\n",
    "Answer:\"\"\"\n",
    "    \n",
    "    # Prepare request body for Claude\n",
    "    request_body = json.dumps({\n",
    "        \"anthropic_version\": \"bedrock-2023-05-31\",\n",
    "        \"max_tokens\": 1000,\n",
    "        \"temperature\": 0.7,\n",
    "        \"messages\": [\n",
    "            {\n",
    "                \"role\": \"user\",\n",
    "                \"content\": prompt\n",
    "            }\n",
    "        ]\n",
    "    })\n",
    "    \n",
    "    try:\n",
    "        # Invoke Claude via Bedrock\n",
    "        response = bedrock_runtime.invoke_model(\n",
    "            modelId='anthropic.claude-3-sonnet-20240229-v1:0',\n",
    "            contentType='application/json',\n",
    "            accept='application/json',\n",
    "            body=request_body\n",
    "        )\n",
    "        \n",
    "        # Parse response\n",
    "        response_body = json.loads(response['body'].read())\n",
    "        answer = response_body['content'][0]['text']\n",
    "        \n",
    "        return answer\n",
    "    \n",
    "    except Exception as e:\n",
    "        return f\"âŒ Error generating answer: {str(e)}\"\n",
    "\n",
    "\n",
    "def ask_question(question: str, k: int = 3) -> Dict:\n",
    "    \"\"\"\n",
    "    Complete RAG pipeline: Retrieve relevant chunks and generate answer\n",
    "    \n",
    "    Args:\n",
    "        question: User's question\n",
    "        k: Number of chunks to retrieve\n",
    "    \n",
    "    Returns:\n",
    "        Dictionary with question, answer, and sources\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(f\"â“ QUESTION: {question}\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    # Step 1: Retrieve relevant chunks\n",
    "    print(\"\\nğŸ“š Step 1: Retrieving relevant context...\")\n",
    "    search_results = search_similar_chunks(question, k=k)\n",
    "    context_chunks = [result['text'] for result in search_results]\n",
    "    \n",
    "    # Step 2: Generate answer with Claude\n",
    "    print(f\"\\nğŸ¤– Step 2: Generating answer with Claude...\")\n",
    "    answer = generate_answer(question, context_chunks)\n",
    "    \n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ’¬ ANSWER:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(answer)\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“– SOURCES USED:\")\n",
    "    print(\"=\" * 80)\n",
    "    for source in search_results:\n",
    "        print(f\"\\nğŸ“Œ Source {source['rank']} (Similarity: {source['similarity_score']:.4f})\")\n",
    "        print(f\"   {source['text'][:150]}...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'sources': search_results\n",
    "    }\n",
    "\n",
    "\n",
    "# Test the complete RAG pipeline\n",
    "print(\"\\nğŸ§ª TESTING COMPLETE RAG PIPELINE\\n\")\n",
    "\n",
    "result = ask_question(\"What is Retrieval Augmented Generation and how does it work?\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e61bdef7-10bd-4244-b82e-51b55b745e9e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ \n",
      "COMPREHENSIVE RAG SYSTEM TEST\n",
      "ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ ğŸ¯ \n",
      "\n",
      "\n",
      "################################################################################\n",
      "TEST 1/5\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "â“ QUESTION: What is AWS Bedrock?\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Step 1: Retrieving relevant context...\n",
      "\n",
      "ğŸ” Searching for: 'What is AWS Bedrock?'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0080, Distance: 123.4524)\n",
      "   Text preview: Key Features of AWS Bedrock:\n",
      "1. Model Choice: Access to multiple foundation models including Claude,...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0080, Distance: 124.5595)\n",
      "   Text preview: AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-per...\n",
      "\n",
      "\n",
      "ğŸ¤– Step 2: Generating answer with Claude...\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¬ ANSWER:\n",
      "================================================================================\n",
      "According to the provided context, AWS Bedrock is:\n",
      "\n",
      "- A fully managed service from Amazon Web Services (AWS)\n",
      "- It offers access to multiple high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon through a single API\n",
      "- It provides capabilities for building generative AI applications with a focus on security, privacy, and responsible AI practices\n",
      "- Key features include:\n",
      "    - Choice of multiple foundation models\n",
      "    - Ability to fine-tune models with your own data (e.g., instruction tuning)\n",
      "    - Enterprise-grade security with AWS PrivateLink support\n",
      "    - Serverless architecture (pay only for what you use)\n",
      "    - Seamless integration with other AWS services\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– SOURCES USED:\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ Source 1 (Similarity: 0.0080)\n",
      "   Key Features of AWS Bedrock:\n",
      "1. Model Choice: Access to multiple foundation models including Claude, Titan, Jurassic, and more\n",
      "2. Customization: Fine-...\n",
      "\n",
      "ğŸ“Œ Source 2 (Similarity: 0.0080)\n",
      "   AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) \n",
      "from leading AI c...\n",
      "================================================================================\n",
      "\n",
      "â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next question... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 2/5\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "â“ QUESTION: What are the key features of Bedrock?\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Step 1: Retrieving relevant context...\n",
      "\n",
      "ğŸ” Searching for: 'What are the key features of Bedrock?'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0057, Distance: 173.9636)\n",
      "   Text preview: Key Features of AWS Bedrock:\n",
      "1. Model Choice: Access to multiple foundation models including Claude,...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0056, Distance: 177.3908)\n",
      "   Text preview: AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-per...\n",
      "\n",
      "\n",
      "ğŸ¤– Step 2: Generating answer with Claude...\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¬ ANSWER:\n",
      "================================================================================\n",
      "According to the provided context, the key features of AWS Bedrock are:\n",
      "\n",
      "- Model Choice: Access to multiple foundation models including Claude, Titan, Jurassic, and more from various AI companies\n",
      "- Customization: Ability to fine-tune models with your own data using techniques like instruction tuning\n",
      "- Security: Enterprise-grade security with support for AWS PrivateLink\n",
      "- Serverless: No infrastructure to manage, pay only for what you use\n",
      "- Integration: Seamless integration with other AWS services\n",
      "\n",
      "The context highlights that Bedrock offers a choice of high-performing foundation models from leading AI companies through a single API, along with capabilities for customization, security, serverless operation, and integration with AWS services.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– SOURCES USED:\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ Source 1 (Similarity: 0.0057)\n",
      "   Key Features of AWS Bedrock:\n",
      "1. Model Choice: Access to multiple foundation models including Claude, Titan, Jurassic, and more\n",
      "2. Customization: Fine-...\n",
      "\n",
      "ğŸ“Œ Source 2 (Similarity: 0.0056)\n",
      "   AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-performing foundation models (FMs) \n",
      "from leading AI c...\n",
      "================================================================================\n",
      "\n",
      "â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next question... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 3/5\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "â“ QUESTION: Explain vector embeddings in simple terms\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Step 1: Retrieving relevant context...\n",
      "\n",
      "ğŸ” Searching for: 'Explain vector embeddings in simple terms'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0054, Distance: 184.9695)\n",
      "   Text preview: Vector Embeddings Explained:\n",
      "Vector embeddings are numerical representations of text that capture se...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0027, Distance: 373.3894)\n",
      "   Text preview: - These documents are selected based on their semantic relevance to the user query, typically by usi...\n",
      "\n",
      "\n",
      "ğŸ¤– Step 2: Generating answer with Claude...\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¬ ANSWER:\n",
      "================================================================================\n",
      "Vector embeddings are a way to represent text (words, sentences, or documents) as numerical vectors (arrays of numbers) in a high-dimensional space. The key aspects are:\n",
      "\n",
      "- Similar concepts or meanings are positioned close together in this vector space.\n",
      "- This allows for:\n",
      "  - Semantic search capabilities (finding relevant documents based on meaning, not just keywords)\n",
      "  - Comparing similarity between texts\n",
      "  - Efficient information retrieval\n",
      "\n",
      "The context provides a good high-level explanation of vector embeddings and their applications, particularly in enabling semantic search and augmenting language models with relevant context. However, it doesn't go into further details or provide a more simplified explanation.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– SOURCES USED:\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ Source 1 (Similarity: 0.0054)\n",
      "   Vector Embeddings Explained:\n",
      "Vector embeddings are numerical representations of text that capture semantic meaning. They convert \n",
      "words, sentences, or...\n",
      "\n",
      "ğŸ“Œ Source 2 (Similarity: 0.0027)\n",
      "   - These documents are selected based on their semantic relevance to the user query, typically by using vector-based search methods (e.g., **dense retr...\n",
      "================================================================================\n",
      "\n",
      "â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next question... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 4/5\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "â“ QUESTION: What are common use cases for RAG systems?\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Step 1: Retrieving relevant context...\n",
      "\n",
      "ğŸ” Searching for: 'What are common use cases for RAG systems?'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0085, Distance: 115.9732)\n",
      "   Text preview: 3. **Knowledge Source Management**:\n",
      "   - Managing and maintaining an up-to-date knowledge base is cr...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0077, Distance: 128.1490)\n",
      "   Text preview: **Applications of RAG**:\n",
      "1. **Customer Support Systems**:\n",
      "   - RAG-powered chatbots can pull relevan...\n",
      "\n",
      "\n",
      "ğŸ¤– Step 2: Generating answer with Claude...\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¬ ANSWER:\n",
      "================================================================================\n",
      "Based on the provided context, some common use cases for RAG (Retrieval-Augmented Generation) systems are:\n",
      "\n",
      "- Customer Support Systems: RAG-powered chatbots can provide personalized answers to customer queries by retrieving relevant information from company documentation, FAQs, and product manuals.\n",
      "\n",
      "- Content Creation: Writers and content creators can leverage RAG systems to generate high-quality articles, blog posts, or marketing copy by retrieving information from multiple sources and generating new content on-demand.\n",
      "\n",
      "The context does not provide any additional use cases beyond these two applications. However, it's worth noting that RAG systems can potentially be useful in any domain or task that requires retrieving and synthesizing information from large knowledge bases to generate relevant and coherent outputs.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– SOURCES USED:\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ Source 1 (Similarity: 0.0085)\n",
      "   3. **Knowledge Source Management**:\n",
      "   - Managing and maintaining an up-to-date knowledge base is crucial for RAG systems. If the knowledge base is ou...\n",
      "\n",
      "ğŸ“Œ Source 2 (Similarity: 0.0077)\n",
      "   **Applications of RAG**:\n",
      "1. **Customer Support Systems**:\n",
      "   - RAG-powered chatbots can pull relevant company documentation and respond to customer qu...\n",
      "================================================================================\n",
      "\n",
      "â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next question... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "################################################################################\n",
      "TEST 5/5\n",
      "################################################################################\n",
      "\n",
      "================================================================================\n",
      "â“ QUESTION: What are best practices for RAG systems?\n",
      "================================================================================\n",
      "\n",
      "ğŸ“š Step 1: Retrieving relevant context...\n",
      "\n",
      "ğŸ” Searching for: 'What are best practices for RAG systems?'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0079, Distance: 125.8281)\n",
      "   Text preview: 3. **Knowledge Source Management**:\n",
      "   - Managing and maintaining an up-to-date knowledge base is cr...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0061, Distance: 163.8999)\n",
      "   Text preview: **Applications of RAG**:\n",
      "1. **Customer Support Systems**:\n",
      "   - RAG-powered chatbots can pull relevan...\n",
      "\n",
      "\n",
      "ğŸ¤– Step 2: Generating answer with Claude...\n",
      "\n",
      "================================================================================\n",
      "ğŸ’¬ ANSWER:\n",
      "================================================================================\n",
      "Based on the provided context, some best practices for RAG systems include:\n",
      "\n",
      "- Knowledge Source Management:\n",
      "  - Maintaining an up-to-date and comprehensive knowledge base to ensure the retrieval of accurate and relevant information.\n",
      "  - Implementing processes to regularly update and curate the knowledge base.\n",
      "\n",
      "- Computational Resources:\n",
      "  - Allocating sufficient computational resources for both retrieval and generation stages to ensure optimal performance.\n",
      "  - Exploring techniques to optimize resource utilization and reduce costs, especially when scaling the system.\n",
      "\n",
      "The context does not provide explicit information on other best practices for RAG systems. However, some additional considerations could include:\n",
      "\n",
      "- Data Quality: Ensuring the quality and relevance of the data sources used to build the knowledge base.\n",
      "- Model Training: Employing robust training strategies to enhance the retrieval and generation capabilities of the RAG model.\n",
      "- Evaluation and Testing: Implementing thorough evaluation and testing procedures to assess the system's performance and identify areas for improvement.\n",
      "- Ethical Considerations: Addressing potential biases, privacy concerns, and ethical implications associated with the use of RAG systems.\n",
      "\n",
      "================================================================================\n",
      "ğŸ“– SOURCES USED:\n",
      "================================================================================\n",
      "\n",
      "ğŸ“Œ Source 1 (Similarity: 0.0079)\n",
      "   3. **Knowledge Source Management**:\n",
      "   - Managing and maintaining an up-to-date knowledge base is crucial for RAG systems. If the knowledge base is ou...\n",
      "\n",
      "ğŸ“Œ Source 2 (Similarity: 0.0061)\n",
      "   **Applications of RAG**:\n",
      "1. **Customer Support Systems**:\n",
      "   - RAG-powered chatbots can pull relevant company documentation and respond to customer qu...\n",
      "================================================================================\n",
      "\n",
      "â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ â¸ï¸ \n"
     ]
    },
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Press Enter to continue to next question... \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "================================================================================\n",
      "âœ… ALL TESTS COMPLETED!\n",
      "================================================================================\n",
      "\n",
      "Total questions processed: 5\n"
     ]
    }
   ],
   "source": [
    "# Cell 11: Test with multiple questions\n",
    "\n",
    "print(\"\\n\" + \"ğŸ¯ \" * 20)\n",
    "print(\"COMPREHENSIVE RAG SYSTEM TEST\")\n",
    "print(\"ğŸ¯ \" * 20 + \"\\n\")\n",
    "\n",
    "# List of test questions\n",
    "test_questions = [\n",
    "    \"What is AWS Bedrock?\",\n",
    "    \"What are the key features of Bedrock?\",\n",
    "    \"Explain vector embeddings in simple terms\",\n",
    "    \"What are common use cases for RAG systems?\",\n",
    "    \"What are best practices for RAG systems?\"\n",
    "]\n",
    "\n",
    "# Store all results\n",
    "all_results = []\n",
    "\n",
    "for i, question in enumerate(test_questions, 1):\n",
    "    print(f\"\\n{'#' * 80}\")\n",
    "    print(f\"TEST {i}/{len(test_questions)}\")\n",
    "    print(f\"{'#' * 80}\")\n",
    "    \n",
    "    result = ask_question(question, k=2)\n",
    "    all_results.append(result)\n",
    "    \n",
    "    print(\"\\n\" + \"â¸ï¸ \" * 40)\n",
    "    input(\"Press Enter to continue to next question...\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… ALL TESTS COMPLETED!\")\n",
    "print(\"=\" * 80)\n",
    "print(f\"\\nTotal questions processed: {len(all_results)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "f04b27a5-742d-4940-8273-294e801017b0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ’¾ Saving RAG system...\n",
      "  âœ… Data saved to: rag-project/rag_system_data.pkl\n",
      "  âœ… Index saved to: rag-project/faiss_index.bin\n",
      "  âœ… Metadata saved to: rag-project/metadata.json\n",
      "\n",
      "âœ… RAG system saved successfully!\n",
      "\n",
      "================================================================================\n",
      "Testing system reload...\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 12: Save RAG system for later use\n",
    "\n",
    "import pickle\n",
    "\n",
    "def save_rag_system(base_path='rag-project'):\n",
    "    \"\"\"Save the complete RAG system state\"\"\"\n",
    "    \n",
    "    print(\"ğŸ’¾ Saving RAG system...\")\n",
    "    \n",
    "    # Save data\n",
    "    data = {\n",
    "        'chunk_texts': chunk_texts,\n",
    "        'embeddings': embeddings,\n",
    "        'doc_store': doc_store\n",
    "    }\n",
    "    \n",
    "    data_path = f'{base_path}/rag_system_data.pkl'\n",
    "    with open(data_path, 'wb') as f:\n",
    "        pickle.dump(data, f)\n",
    "    print(f\"  âœ… Data saved to: {data_path}\")\n",
    "    \n",
    "    # Save FAISS index\n",
    "    index_path = f'{base_path}/faiss_index.bin'\n",
    "    faiss.write_index(index, index_path)\n",
    "    print(f\"  âœ… Index saved to: {index_path}\")\n",
    "    \n",
    "    # Save metadata\n",
    "    metadata = {\n",
    "        'num_chunks': len(chunk_texts),\n",
    "        'embedding_dimension': len(embeddings[0]),\n",
    "        'index_type': 'IndexFlatL2',\n",
    "        'model_ids': {\n",
    "            'embedding': 'amazon.titan-embed-text-v1',\n",
    "            'llm': 'anthropic.claude-3-sonnet-20240229-v1:0'\n",
    "        }\n",
    "    }\n",
    "    \n",
    "    metadata_path = f'{base_path}/metadata.json'\n",
    "    with open(metadata_path, 'w') as f:\n",
    "        json.dump(metadata, f, indent=2)\n",
    "    print(f\"  âœ… Metadata saved to: {metadata_path}\")\n",
    "    \n",
    "    print(\"\\nâœ… RAG system saved successfully!\")\n",
    "    return True\n",
    "\n",
    "\n",
    "def load_rag_system(base_path='rag-project'):\n",
    "    \"\"\"Load the RAG system from disk\"\"\"\n",
    "    \n",
    "    global chunk_texts, embeddings, doc_store, index\n",
    "    \n",
    "    print(\"ğŸ“‚ Loading RAG system...\")\n",
    "    \n",
    "    try:\n",
    "        # Load data\n",
    "        data_path = f'{base_path}/rag_system_data.pkl'\n",
    "        with open(data_path, 'rb') as f:\n",
    "            data = pickle.load(f)\n",
    "        \n",
    "        chunk_texts = data['chunk_texts']\n",
    "        embeddings = data['embeddings']\n",
    "        doc_store = data['doc_store']\n",
    "        print(f\"  âœ… Data loaded from: {data_path}\")\n",
    "        \n",
    "        # Load FAISS index\n",
    "        index_path = f'{base_path}/faiss_index.bin'\n",
    "        index = faiss.read_index(index_path)\n",
    "        print(f\"  âœ… Index loaded from: {index_path}\")\n",
    "        \n",
    "        # Load metadata\n",
    "        metadata_path = f'{base_path}/metadata.json'\n",
    "        with open(metadata_path, 'r') as f:\n",
    "            metadata = json.load(f)\n",
    "        \n",
    "        print(f\"  âœ… Metadata loaded from: {metadata_path}\")\n",
    "        print(f\"\\nğŸ“Š System Info:\")\n",
    "        print(f\"   - Chunks: {metadata['num_chunks']}\")\n",
    "        print(f\"   - Embedding dimension: {metadata['embedding_dimension']}\")\n",
    "        print(f\"   - Embedding model: {metadata['model_ids']['embedding']}\")\n",
    "        print(f\"   - LLM model: {metadata['model_ids']['llm']}\")\n",
    "        \n",
    "        print(\"\\nâœ… RAG system loaded successfully!\")\n",
    "        return True\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"âŒ Error loading RAG system: {e}\")\n",
    "        return False\n",
    "\n",
    "\n",
    "# Save current system\n",
    "save_rag_system()\n",
    "\n",
    "# Test loading (optional)\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"Testing system reload...\")\n",
    "print(\"=\" * 80)\n",
    "# Uncomment to test:\n",
    "# load_rag_system()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "068a079f-83b9-43a2-9ead-9ae4754f3f67",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ğŸ” SYSTEM HEALTH CHECK\n",
      "================================================================================\n",
      "\n",
      "1ï¸âƒ£ Document Loading:\n",
      "   âœ… Documents loaded: 1\n",
      "   âœ… Total characters: 7,483\n",
      "\n",
      "2ï¸âƒ£ Document Chunking:\n",
      "   âœ… Chunks created: 20\n",
      "   âœ… Average chunk size: 376 chars\n",
      "\n",
      "3ï¸âƒ£ Vector Embeddings:\n",
      "   âœ… Embeddings generated: 20\n",
      "   âœ… Embedding dimension: 1536\n",
      "\n",
      "4ï¸âƒ£ Vector Store:\n",
      "   âœ… FAISS index created: 20 vectors\n",
      "   âœ… Doc store mapping: 20 entries\n",
      "\n",
      "5ï¸âƒ£ AWS Bedrock Connection:\n",
      "   âœ… Bedrock embedding working\n",
      "\n",
      "6ï¸âƒ£ Vector Search:\n",
      "\n",
      "ğŸ” Searching for: 'test query'\n",
      "ğŸ“Š Retrieving top 1 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0016, Distance: 612.0904)\n",
      "   Text preview: - These documents are selected based on their semantic relevance to the user query, typically by usi...\n",
      "\n",
      "   âœ… Vector search working\n",
      "\n",
      "7ï¸âƒ£ Claude LLM:\n",
      "   âœ… Claude generation working\n",
      "\n",
      "================================================================================\n",
      "âœ… HEALTH CHECK COMPLETE!\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 13: Complete system verification\n",
    "\n",
    "# Cell 13: Complete system verification\n",
    "\n",
    "print(\"ğŸ” SYSTEM HEALTH CHECK\")\n",
    "print(\"=\" * 80)\n",
    "\n",
    "# Check 1: Documents loaded\n",
    "print(\"\\n1ï¸âƒ£ Document Loading:\")\n",
    "try:\n",
    "    print(f\"   âœ… Documents loaded: {len(documents)}\")\n",
    "    print(f\"   âœ… Total characters: {sum(len(doc.page_content) for doc in documents):,}\")\n",
    "except:\n",
    "    print(\"   âŒ Documents not loaded\")\n",
    "\n",
    "# Check 2: Chunks created\n",
    "print(\"\\n2ï¸âƒ£ Document Chunking:\")\n",
    "try:\n",
    "    print(f\"   âœ… Chunks created: {len(chunks)}\")\n",
    "    print(f\"   âœ… Average chunk size: {sum(len(c.page_content) for c in chunks) // len(chunks)} chars\")\n",
    "except:\n",
    "    print(\"   âŒ Chunks not created\")\n",
    "\n",
    "# Check 3: Embeddings generated\n",
    "print(\"\\n3ï¸âƒ£ Vector Embeddings:\")\n",
    "try:\n",
    "    print(f\"   âœ… Embeddings generated: {len(embeddings)}\")\n",
    "    print(f\"   âœ… Embedding dimension: {len(embeddings[0])}\")\n",
    "except:\n",
    "    print(\"   âŒ Embeddings not generated\")\n",
    "\n",
    "# Check 4: FAISS index\n",
    "print(\"\\n4ï¸âƒ£ Vector Store:\")\n",
    "try:\n",
    "    print(f\"   âœ… FAISS index created: {index.ntotal} vectors\")\n",
    "    print(f\"   âœ… Doc store mapping: {len(doc_store)} entries\")\n",
    "except:\n",
    "    print(\"   âŒ Vector store not created\")\n",
    "\n",
    "# Check 5: Bedrock connection\n",
    "print(\"\\n5ï¸âƒ£ AWS Bedrock Connection:\")\n",
    "try:\n",
    "    test_embedding = get_embedding(\"test\")\n",
    "    if test_embedding:\n",
    "        print(f\"   âœ… Bedrock embedding working\")\n",
    "    else:\n",
    "        print(f\"   âŒ Bedrock embedding failed\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Bedrock connection error: {e}\")\n",
    "\n",
    "# Check 6: Search function\n",
    "print(\"\\n6ï¸âƒ£ Vector Search:\")\n",
    "try:\n",
    "    test_results = search_similar_chunks(\"test query\", k=1)\n",
    "    if test_results:\n",
    "        print(f\"   âœ… Vector search working\")\n",
    "    else:\n",
    "        print(f\"   âŒ Vector search failed\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ Search error: {e}\")\n",
    "\n",
    "# Check 7: LLM generation\n",
    "print(\"\\n7ï¸âƒ£ Claude LLM:\")\n",
    "try:\n",
    "    test_answer = generate_answer(\"What is AI?\", [\"AI is artificial intelligence\"])\n",
    "    if test_answer and not test_answer.startswith(\"âŒ\"):\n",
    "        print(f\"   âœ… Claude generation working\")\n",
    "    else:\n",
    "        print(f\"   âŒ Claude generation failed\")\n",
    "except Exception as e:\n",
    "    print(f\"   âŒ LLM error: {e}\")\n",
    "\n",
    "print(\"\\n\" + \"=\" * 80)\n",
    "print(\"âœ… HEALTH CHECK COMPLETE!\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2cafb270-07c2-44f5-ae03-814b07ec0478",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·\n",
      "â“ Question: What are the best practices for RAG systems?\n",
      "ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·\n",
      "\n",
      "ğŸ” Searching knowledge base...\n",
      "\n",
      "ğŸ” Searching for: 'What are the best practices for RAG systems?'\n",
      "ğŸ“Š Retrieving top 3 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0082, Distance: 120.8745)\n",
      "   Text preview: 3. **Knowledge Source Management**:\n",
      "   - Managing and maintaining an up-to-date knowledge base is cr...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0063, Distance: 158.1544)\n",
      "   Text preview: **Applications of RAG**:\n",
      "1. **Customer Support Systems**:\n",
      "   - RAG-powered chatbots can pull relevan...\n",
      "\n",
      "ğŸ“Œ Result 3 (Score: 0.0063, Distance: 158.2815)\n",
      "   Text preview: **Why Use RAG?**\n",
      "1. **Combining Retrieval and Generation**:\n",
      "   - RAG combines the **precision of inf...\n",
      "\n",
      "ğŸ¤– Generating answer with Claude...\n",
      "\n",
      "ğŸ’¬ ANSWER:\n",
      "--------------------------------------------------------------------------------\n",
      "Based on the provided context, some best practices for RAG (Retrieval-Augmented Generation) systems include:\n",
      "\n",
      "- **Knowledge Source Management**: Maintaining an up-to-date and comprehensive knowledge base is crucial to ensure the system retrieves accurate and relevant information. Outdated or incomplete knowledge sources can lead to incorrect or suboptimal answers.\n",
      "\n",
      "- **Computational Resource Management**: RAG systems often require significant computational resources for retrieval and generation stages. Proper resource allocation and optimization strategies should be implemented, especially when scaling the system.\n",
      "\n",
      "- **Combining Retrieval and Generation**: Leverage the strengths of both information retrieval (precision) and generative models (creativity) to create a more flexible and powerful AI system capable of retrieving accurate information and generating relevant answers based on the latest data.\n",
      "\n",
      "The context does not provide explicit details on other best practices, such as techniques for improving retrieval quality, handling knowledge source updates, or fine-tuning the generative model for specific domains. If more comprehensive best practices are required, additional information sources may be needed.\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ“š SOURCES:\n",
      "\n",
      "[1] Relevance: 0.82%\n",
      "    3. **Knowledge Source Management**:\n",
      "   - Managing and maintaining an up-to-date knowledge base is crucial for RAG systems. If the knowledge base is outdated or incomplete, the model might retrieve out...\n",
      "\n",
      "[2] Relevance: 0.63%\n",
      "    **Applications of RAG**:\n",
      "1. **Customer Support Systems**:\n",
      "   - RAG-powered chatbots can pull relevant company documentation and respond to customer queries by providing personalized answers based on c...\n",
      "\n",
      "[3] Relevance: 0.63%\n",
      "    **Why Use RAG?**\n",
      "1. **Combining Retrieval and Generation**:\n",
      "   - RAG combines the **precision of information retrieval** with the **creativity of generative models**, enabling a more flexible and powe...\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'question': 'What are the best practices for RAG systems?',\n",
       " 'answer': 'Based on the provided context, some best practices for RAG (Retrieval-Augmented Generation) systems include:\\n\\n- **Knowledge Source Management**: Maintaining an up-to-date and comprehensive knowledge base is crucial to ensure the system retrieves accurate and relevant information. Outdated or incomplete knowledge sources can lead to incorrect or suboptimal answers.\\n\\n- **Computational Resource Management**: RAG systems often require significant computational resources for retrieval and generation stages. Proper resource allocation and optimization strategies should be implemented, especially when scaling the system.\\n\\n- **Combining Retrieval and Generation**: Leverage the strengths of both information retrieval (precision) and generative models (creativity) to create a more flexible and powerful AI system capable of retrieving accurate information and generating relevant answers based on the latest data.\\n\\nThe context does not provide explicit details on other best practices, such as techniques for improving retrieval quality, handling knowledge source updates, or fine-tuning the generative model for specific domains. If more comprehensive best practices are required, additional information sources may be needed.',\n",
       " 'sources': [{'rank': 1,\n",
       "   'index': 14,\n",
       "   'text': '3. **Knowledge Source Management**:\\n   - Managing and maintaining an up-to-date knowledge base is crucial for RAG systems. If the knowledge base is outdated or incomplete, the model might retrieve outdated information, leading to incorrect or suboptimal answers.\\n\\n4. **Computational Resources**:\\n   - RAG systems often require significant computational resources for both retrieval and generation stages, making them resource-intensive. This can result in higher costs, especially when scaled.',\n",
       "   'distance': 120.87451171875,\n",
       "   'similarity_score': 0.008205161078369706},\n",
       "  {'rank': 2,\n",
       "   'index': 9,\n",
       "   'text': '**Applications of RAG**:\\n1. **Customer Support Systems**:\\n   - RAG-powered chatbots can pull relevant company documentation and respond to customer queries by providing personalized answers based on current product data, FAQs, and manuals.\\n   \\n2. **Content Creation**:\\n   - Writers and content creators can use RAG to quickly generate high-quality, relevant articles, blog posts, or marketing copy by retrieving information from multiple sources and generating new content on-demand.',\n",
       "   'distance': 158.15438842773438,\n",
       "   'similarity_score': 0.006283207204519276},\n",
       "  {'rank': 3,\n",
       "   'index': 6,\n",
       "   'text': '**Why Use RAG?**\\n1. **Combining Retrieval and Generation**:\\n   - RAG combines the **precision of information retrieval** with the **creativity of generative models**, enabling a more flexible and powerful AI system. \\n   - This allows for a scalable solution where the AI can retrieve accurate information from a large set of documents and generate answers based on the latest available data.\\n   \\n2. **Improving Model Accuracy**:',\n",
       "   'distance': 158.28152465820312,\n",
       "   'similarity_score': 0.006278192038567351}]}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Cell 14: Advanced Query Function with Sources Display\n",
    "\n",
    "def ask_with_sources(question: str, k: int = 3, show_sources: bool = True):\n",
    "    \"\"\"\n",
    "    Ask a question and get answer with detailed source attribution\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"ğŸ”·\" * 40)\n",
    "    print(f\"â“ Question: {question}\")\n",
    "    print(\"ğŸ”·\" * 40)\n",
    "    \n",
    "    # Retrieve\n",
    "    print(\"\\nğŸ” Searching knowledge base...\")\n",
    "    search_results = search_similar_chunks(question, k=k)\n",
    "    context_chunks = [result['text'] for result in search_results]\n",
    "    \n",
    "    # Generate\n",
    "    print(\"ğŸ¤– Generating answer with Claude...\\n\")\n",
    "    answer = generate_answer(question, context_chunks)\n",
    "    \n",
    "    # Display answer\n",
    "    print(\"ğŸ’¬ ANSWER:\")\n",
    "    print(\"-\" * 80)\n",
    "    print(answer)\n",
    "    print(\"-\" * 80)\n",
    "    \n",
    "    # Display sources if requested\n",
    "    if show_sources:\n",
    "        print(\"\\nğŸ“š SOURCES:\")\n",
    "        for i, result in enumerate(search_results, 1):\n",
    "            print(f\"\\n[{i}] Relevance: {result['similarity_score']:.2%}\")\n",
    "            print(f\"    {result['text'][:200]}...\")\n",
    "    \n",
    "    return {\n",
    "        'question': question,\n",
    "        'answer': answer,\n",
    "        'sources': search_results\n",
    "    }\n",
    "\n",
    "# Test it\n",
    "ask_with_sources(\"What are the best practices for RAG systems?\", k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "f85bd80d-44fc-49c7-b241-7122fab65fbf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ”„ Processing 4 questions...\n",
      "================================================================================\n",
      "\n",
      "[1/4] What is AWS Bedrock?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·\n",
      "â“ Question: What is AWS Bedrock?\n",
      "ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·\n",
      "\n",
      "ğŸ” Searching knowledge base...\n",
      "\n",
      "ğŸ” Searching for: 'What is AWS Bedrock?'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0080, Distance: 123.4524)\n",
      "   Text preview: Key Features of AWS Bedrock:\n",
      "1. Model Choice: Access to multiple foundation models including Claude,...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0080, Distance: 124.5595)\n",
      "   Text preview: AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-per...\n",
      "\n",
      "ğŸ¤– Generating answer with Claude...\n",
      "\n",
      "ğŸ’¬ ANSWER:\n",
      "--------------------------------------------------------------------------------\n",
      "According to the provided context, AWS Bedrock is:\n",
      "\n",
      "- A fully managed service from AWS that provides access to multiple high-performing foundation models (FMs) from leading AI companies like AI21 Labs, Anthropic, Cohere, Meta, Stability AI, and Amazon through a single API.\n",
      "- It offers capabilities to build generative AI applications with:\n",
      "    - Security and privacy features\n",
      "    - Responsible AI practices\n",
      "- Key features include:\n",
      "    - Model choice: Access to multiple FMs like Claude, Titan, Jurassic, and more\n",
      "    - Customization: Fine-tuning models with your own data using techniques like instruction tuning\n",
      "    - Enterprise-grade security with AWS PrivateLink support\n",
      "    - Serverless architecture, pay only for what you use\n",
      "    - Seamless integration with other AWS services\n",
      "--------------------------------------------------------------------------------\n",
      "âœ… Completed\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[2/4] How does RAG work?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·\n",
      "â“ Question: How does RAG work?\n",
      "ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·\n",
      "\n",
      "ğŸ” Searching knowledge base...\n",
      "\n",
      "ğŸ” Searching for: 'How does RAG work?'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0076, Distance: 130.3964)\n",
      "   Text preview: **Why Use RAG?**\n",
      "1. **Combining Retrieval and Generation**:\n",
      "   - RAG combines the **precision of inf...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0073, Distance: 136.4141)\n",
      "   Text preview: **Applications of RAG**:\n",
      "1. **Customer Support Systems**:\n",
      "   - RAG-powered chatbots can pull relevan...\n",
      "\n",
      "ğŸ¤– Generating answer with Claude...\n",
      "\n",
      "ğŸ’¬ ANSWER:\n",
      "--------------------------------------------------------------------------------\n",
      "The provided context does not explicitly explain how RAG (Retrieval-Augmented Generation) works. However, based on the information given, we can infer the following about RAG's working:\n",
      "\n",
      "- RAG combines two key components:\n",
      "  - Information Retrieval: It has the capability to retrieve relevant information from a large set of documents or data sources.\n",
      "  - Generative Model: It uses a language generation model that can produce human-like text based on the retrieved information.\n",
      "\n",
      "- The general process likely involves:\n",
      "  1. Receiving a query or input text\n",
      "  2. Using an information retrieval system to identify and retrieve relevant documents or passages from the available data sources\n",
      "  3. Feeding the retrieved information into a generative language model\n",
      "  4. The language model generates a coherent and contextualized response by combining and interpreting the retrieved information\n",
      "\n",
      "- RAG leverages the strengths of both retrieval and generation, allowing it to provide accurate and up-to-date information while maintaining the flexibility and creativity of generative models.\n",
      "\n",
      "The context does not provide more specific technical details about the algorithms, architectures, or training methods used in RAG systems.\n",
      "--------------------------------------------------------------------------------\n",
      "âœ… Completed\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[3/4] What are vector embeddings?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·\n",
      "â“ Question: What are vector embeddings?\n",
      "ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·\n",
      "\n",
      "ğŸ” Searching knowledge base...\n",
      "\n",
      "ğŸ” Searching for: 'What are vector embeddings?'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0071, Distance: 139.9378)\n",
      "   Text preview: Vector Embeddings Explained:\n",
      "Vector embeddings are numerical representations of text that capture se...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0027, Distance: 364.0360)\n",
      "   Text preview: - These documents are selected based on their semantic relevance to the user query, typically by usi...\n",
      "\n",
      "ğŸ¤– Generating answer with Claude...\n",
      "\n",
      "ğŸ’¬ ANSWER:\n",
      "--------------------------------------------------------------------------------\n",
      "According to the provided context, vector embeddings are:\n",
      "\n",
      "- Numerical representations of text (words, sentences, or documents) that capture semantic meaning.\n",
      "- They convert text into arrays of numbers (vectors) in a high-dimensional space.\n",
      "- Similar concepts are placed close together in this vector space, enabling:\n",
      "  - Semantic search capabilities\n",
      "  - Similarity comparisons\n",
      "  - Efficient information retrieval\n",
      "- Vector embeddings allow for techniques like dense retrieval and semantic search, where relevant documents are selected based on their semantic similarity to the user's query.\n",
      "\n",
      "The context provides a clear explanation of what vector embeddings are and their primary use cases related to semantic search and information retrieval. It does not go into further technical details about how vector embeddings are generated or the specific algorithms used.\n",
      "--------------------------------------------------------------------------------\n",
      "âœ… Completed\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "[4/4] What are common use cases?\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·\n",
      "â“ Question: What are common use cases?\n",
      "ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·ğŸ”·\n",
      "\n",
      "ğŸ” Searching knowledge base...\n",
      "\n",
      "ğŸ” Searching for: 'What are common use cases?'\n",
      "ğŸ“Š Retrieving top 2 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0040, Distance: 251.3959)\n",
      "   Text preview: Common Use Cases:\n",
      "1. Intelligent Chatbots: Customer service bots that access company documentation\n",
      "2...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0037, Distance: 267.7300)\n",
      "   Text preview: 3. **Legal and Medical Document Analysis**:\n",
      "   - RAG can be used to process large legal or medical d...\n",
      "\n",
      "ğŸ¤– Generating answer with Claude...\n",
      "\n",
      "ğŸ’¬ ANSWER:\n",
      "--------------------------------------------------------------------------------\n",
      "Based on the provided context, some common use cases for RAG (Retrieval-Augmented Generation) models are:\n",
      "\n",
      "- Intelligent Chatbots for customer service that can access and utilize company documentation\n",
      "- Document Analysis, including automated summarization and question-answering over large document sets\n",
      "- Content Generation, such as creating marketing copy, reports, or creative content\n",
      "- Code Assistance, generating and explaining code snippets\n",
      "- Research Tools to help researchers find relevant papers and information\n",
      "- Legal and Medical Document Analysis, providing summaries or answers to specific questions based on the contents of large legal or medical document sets\n",
      "--------------------------------------------------------------------------------\n",
      "âœ… Completed\n",
      "--------------------------------------------------------------------------------\n",
      "\n",
      "âœ… Processed 4 questions successfully!\n"
     ]
    }
   ],
   "source": [
    "# Cell 15: Batch Question Processing\n",
    "\n",
    "def process_multiple_questions(questions: List[str], k: int = 3):\n",
    "    \"\"\"\n",
    "    Process multiple questions and return all results\n",
    "    \"\"\"\n",
    "    results = []\n",
    "    \n",
    "    print(f\"\\nğŸ”„ Processing {len(questions)} questions...\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    for i, question in enumerate(questions, 1):\n",
    "        print(f\"\\n[{i}/{len(questions)}] {question}\")\n",
    "        print(\"-\" * 80)\n",
    "        \n",
    "        try:\n",
    "            result = ask_with_sources(question, k=k, show_sources=False)\n",
    "            results.append(result)\n",
    "            print(\"âœ… Completed\")\n",
    "        except Exception as e:\n",
    "            print(f\"âŒ Error: {e}\")\n",
    "            results.append({\n",
    "                'question': question,\n",
    "                'answer': f\"Error: {e}\",\n",
    "                'sources': []\n",
    "            })\n",
    "        \n",
    "        print(\"-\" * 80)\n",
    "    \n",
    "    print(f\"\\nâœ… Processed {len(results)} questions successfully!\")\n",
    "    return results\n",
    "\n",
    "# Example usage\n",
    "batch_questions = [\n",
    "    \"What is AWS Bedrock?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"What are vector embeddings?\",\n",
    "    \"What are common use cases?\"\n",
    "]\n",
    "\n",
    "batch_results = process_multiple_questions(batch_questions, k=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "8060e018-f701-4beb-9e40-869b0c5e8aaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ’¾ Exporting results to rag-project/qa_results.txt...\n",
      "âœ… Results exported to: rag-project/qa_results.txt\n",
      "âœ… JSON version saved to: rag-project/qa_results.json\n"
     ]
    }
   ],
   "source": [
    "# Cell 16: Export Results to File\n",
    "\n",
    "def export_results_to_file(results: List[Dict], filename: str = 'rag-project/qa_results.txt'):\n",
    "    \"\"\"\n",
    "    Export Q&A results to a text file\n",
    "    \"\"\"\n",
    "    print(f\"\\nğŸ’¾ Exporting results to {filename}...\")\n",
    "    \n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        f.write(\"=\" * 80 + \"\\n\")\n",
    "        f.write(\"RAG SYSTEM - Q&A RESULTS\\n\")\n",
    "        f.write(\"=\" * 80 + \"\\n\\n\")\n",
    "        \n",
    "        for i, result in enumerate(results, 1):\n",
    "            f.write(f\"\\n{'='*80}\\n\")\n",
    "            f.write(f\"Q{i}: {result['question']}\\n\")\n",
    "            f.write(f\"{'='*80}\\n\\n\")\n",
    "            f.write(f\"ANSWER:\\n{result['answer']}\\n\\n\")\n",
    "            \n",
    "            if result.get('sources'):\n",
    "                f.write(\"SOURCES:\\n\")\n",
    "                for j, source in enumerate(result['sources'], 1):\n",
    "                    f.write(f\"\\n[{j}] Relevance: {source['similarity_score']:.2%}\\n\")\n",
    "                    f.write(f\"    {source['text'][:200]}...\\n\")\n",
    "            \n",
    "            f.write(\"\\n\" + \"-\"*80 + \"\\n\")\n",
    "    \n",
    "    print(f\"âœ… Results exported to: {filename}\")\n",
    "    \n",
    "    # Also create JSON version\n",
    "    json_filename = filename.replace('.txt', '.json')\n",
    "    with open(json_filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(results, f, indent=2, ensure_ascii=False)\n",
    "    print(f\"âœ… JSON version saved to: {json_filename}\")\n",
    "\n",
    "# Export batch results\n",
    "if 'batch_results' in locals():\n",
    "    export_results_to_file(batch_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "37a8ae44-ffdb-49a0-8baf-faaecbd9984c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cell 17: Interactive Chat Mode\n",
    "\n",
    "def interactive_chat():\n",
    "    \"\"\"\n",
    "    Start an interactive chat session with the RAG system\n",
    "    \"\"\"\n",
    "    print(\"\\n\" + \"ğŸ¤–\" * 40)\n",
    "    print(\"RAG CHATBOT - Interactive Mode\")\n",
    "    print(\"ğŸ¤–\" * 40)\n",
    "    print(\"\\nCommands:\")\n",
    "    print(\"  - Type your question and press Enter\")\n",
    "    print(\"  - Type 'quit' or 'exit' to stop\")\n",
    "    print(\"  - Type 'save' to save conversation\")\n",
    "    print(\"  - Type 'help' for this message\")\n",
    "    print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "    \n",
    "    conversation_history = []\n",
    "    \n",
    "    while True:\n",
    "        try:\n",
    "            # Get user input\n",
    "            user_input = input(\"You: \").strip()\n",
    "            \n",
    "            # Handle commands\n",
    "            if user_input.lower() in ['quit', 'exit', 'q']:\n",
    "                print(\"\\nğŸ‘‹ Thanks for chatting! Goodbye!\")\n",
    "                break\n",
    "            \n",
    "            if user_input.lower() == 'help':\n",
    "                print(\"\\nCommands: quit/exit, save, help\")\n",
    "                continue\n",
    "            \n",
    "            if user_input.lower() == 'save':\n",
    "                if conversation_history:\n",
    "                    export_results_to_file(\n",
    "                        conversation_history, \n",
    "                        'rag-project/chat_history.txt'\n",
    "                    )\n",
    "                else:\n",
    "                    print(\"No conversation to save yet.\")\n",
    "                continue\n",
    "            \n",
    "            if not user_input:\n",
    "                continue\n",
    "            \n",
    "            # Process question\n",
    "            print()\n",
    "            result = ask_with_sources(user_input, k=3, show_sources=True)\n",
    "            conversation_history.append(result)\n",
    "            print(\"\\n\" + \"=\" * 80 + \"\\n\")\n",
    "            \n",
    "        except KeyboardInterrupt:\n",
    "            print(\"\\n\\nğŸ‘‹ Chat interrupted. Goodbye!\")\n",
    "            break\n",
    "        except Exception as e:\n",
    "            print(f\"\\nâŒ Error: {e}\\n\")\n",
    "    \n",
    "    return conversation_history\n",
    "\n",
    "# Uncomment to start interactive chat\n",
    "# chat_history = interactive_chat()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "6382fc0b-4131-49d0-a0a5-2921dc1b29c5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ“Š PERFORMANCE EVALUATION\n",
      "================================================================================\n",
      "\n",
      "[1/5] Testing: What is AWS Bedrock?...\n",
      "\n",
      "ğŸ” Searching for: 'What is AWS Bedrock?'\n",
      "ğŸ“Š Retrieving top 3 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0080, Distance: 123.4524)\n",
      "   Text preview: Key Features of AWS Bedrock:\n",
      "1. Model Choice: Access to multiple foundation models including Claude,...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0080, Distance: 124.5595)\n",
      "   Text preview: AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-per...\n",
      "\n",
      "ğŸ“Œ Result 3 (Score: 0.0030, Distance: 337.7154)\n",
      "   Text preview: Security Considerations:\n",
      "- Always use IAM roles with least privilege\n",
      "- Enable encryption at rest and...\n",
      "\n",
      "   âœ… Success (2.97s)\n",
      "\n",
      "[2/5] Testing: How does RAG work?...\n",
      "\n",
      "ğŸ” Searching for: 'How does RAG work?'\n",
      "ğŸ“Š Retrieving top 3 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0076, Distance: 130.3964)\n",
      "   Text preview: **Why Use RAG?**\n",
      "1. **Combining Retrieval and Generation**:\n",
      "   - RAG combines the **precision of inf...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0073, Distance: 136.4141)\n",
      "   Text preview: **Applications of RAG**:\n",
      "1. **Customer Support Systems**:\n",
      "   - RAG-powered chatbots can pull relevan...\n",
      "\n",
      "ğŸ“Œ Result 3 (Score: 0.0072, Distance: 137.1158)\n",
      "   Text preview: **How RAG Works**:\n",
      "RAG combines the power of retrieval-based techniques and generative language mode...\n",
      "\n",
      "   âœ… Success (3.44s)\n",
      "\n",
      "[3/5] Testing: What are vector embeddings?...\n",
      "\n",
      "ğŸ” Searching for: 'What are vector embeddings?'\n",
      "ğŸ“Š Retrieving top 3 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0071, Distance: 139.9378)\n",
      "   Text preview: Vector Embeddings Explained:\n",
      "Vector embeddings are numerical representations of text that capture se...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0027, Distance: 364.0360)\n",
      "   Text preview: - These documents are selected based on their semantic relevance to the user query, typically by usi...\n",
      "\n",
      "ğŸ“Œ Result 3 (Score: 0.0025, Distance: 393.5494)\n",
      "   Text preview: AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-per...\n",
      "\n",
      "   âœ… Success (2.58s)\n",
      "\n",
      "[4/5] Testing: Explain best practices for RAG...\n",
      "\n",
      "ğŸ” Searching for: 'Explain best practices for RAG'\n",
      "ğŸ“Š Retrieving top 3 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0064, Distance: 154.2135)\n",
      "   Text preview: 3. **Knowledge Source Management**:\n",
      "   - Managing and maintaining an up-to-date knowledge base is cr...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0063, Distance: 158.2602)\n",
      "   Text preview: **Why Use RAG?**\n",
      "1. **Combining Retrieval and Generation**:\n",
      "   - RAG combines the **precision of inf...\n",
      "\n",
      "ğŸ“Œ Result 3 (Score: 0.0061, Distance: 164.2032)\n",
      "   Text preview: Best Practices for RAG Systems:\n",
      "- **Chunk Documents Appropriately** (300-500 tokens): Ensures that e...\n",
      "\n",
      "   âœ… Success (3.36s)\n",
      "\n",
      "[5/5] Testing: What are common use cases for Bedrock?...\n",
      "\n",
      "ğŸ” Searching for: 'What are common use cases for Bedrock?'\n",
      "ğŸ“Š Retrieving top 3 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0057, Distance: 173.4935)\n",
      "   Text preview: AWS Bedrock: Complete Guide\n",
      "\n",
      "AWS Bedrock is a fully managed service that offers a choice of high-per...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0052, Distance: 191.0668)\n",
      "   Text preview: Key Features of AWS Bedrock:\n",
      "1. Model Choice: Access to multiple foundation models including Claude,...\n",
      "\n",
      "ğŸ“Œ Result 3 (Score: 0.0032, Distance: 307.4541)\n",
      "   Text preview: Common Use Cases:\n",
      "1. Intelligent Chatbots: Customer service bots that access company documentation\n",
      "2...\n",
      "\n",
      "   âœ… Success (1.96s)\n",
      "\n",
      "================================================================================\n",
      "ğŸ“ˆ RESULTS:\n",
      "================================================================================\n",
      "Total Questions:        5\n",
      "Successful:             5 (100.0%)\n",
      "Failed:                 0\n",
      "Total Time:             14.30s\n",
      "Average Time/Question:  2.86s\n",
      "  - Retrieval:          0.11s\n",
      "  - Generation:         2.75s\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 18: Performance Metrics\n",
    "\n",
    "import time\n",
    "\n",
    "def evaluate_rag_performance(test_questions: List[str], k: int = 3):\n",
    "    \"\"\"\n",
    "    Evaluate RAG system performance\n",
    "    \"\"\"\n",
    "    print(\"\\nğŸ“Š PERFORMANCE EVALUATION\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    metrics = {\n",
    "        'total_questions': len(test_questions),\n",
    "        'successful': 0,\n",
    "        'failed': 0,\n",
    "        'total_time': 0,\n",
    "        'avg_time': 0,\n",
    "        'retrieval_times': [],\n",
    "        'generation_times': []\n",
    "    }\n",
    "    \n",
    "    for i, question in enumerate(test_questions, 1):\n",
    "        print(f\"\\n[{i}/{len(test_questions)}] Testing: {question[:50]}...\")\n",
    "        \n",
    "        try:\n",
    "            start_time = time.time()\n",
    "            \n",
    "            # Retrieval\n",
    "            retrieval_start = time.time()\n",
    "            search_results = search_similar_chunks(question, k=k)\n",
    "            retrieval_time = time.time() - retrieval_start\n",
    "            metrics['retrieval_times'].append(retrieval_time)\n",
    "            \n",
    "            # Generation\n",
    "            generation_start = time.time()\n",
    "            context = [r['text'] for r in search_results]\n",
    "            answer = generate_answer(question, context)\n",
    "            generation_time = time.time() - generation_start\n",
    "            metrics['generation_times'].append(generation_time)\n",
    "            \n",
    "            total_time = time.time() - start_time\n",
    "            metrics['total_time'] += total_time\n",
    "            \n",
    "            if answer and not answer.startswith(\"âŒ\"):\n",
    "                metrics['successful'] += 1\n",
    "                print(f\"   âœ… Success ({total_time:.2f}s)\")\n",
    "            else:\n",
    "                metrics['failed'] += 1\n",
    "                print(f\"   âŒ Failed ({total_time:.2f}s)\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            metrics['failed'] += 1\n",
    "            print(f\"   âŒ Error: {e}\")\n",
    "    \n",
    "    # Calculate averages\n",
    "    metrics['avg_time'] = metrics['total_time'] / len(test_questions)\n",
    "    metrics['avg_retrieval_time'] = np.mean(metrics['retrieval_times'])\n",
    "    metrics['avg_generation_time'] = np.mean(metrics['generation_times'])\n",
    "    metrics['success_rate'] = (metrics['successful'] / metrics['total_questions']) * 100\n",
    "    \n",
    "    # Display results\n",
    "    print(\"\\n\" + \"=\" * 80)\n",
    "    print(\"ğŸ“ˆ RESULTS:\")\n",
    "    print(\"=\" * 80)\n",
    "    print(f\"Total Questions:        {metrics['total_questions']}\")\n",
    "    print(f\"Successful:             {metrics['successful']} ({metrics['success_rate']:.1f}%)\")\n",
    "    print(f\"Failed:                 {metrics['failed']}\")\n",
    "    print(f\"Total Time:             {metrics['total_time']:.2f}s\")\n",
    "    print(f\"Average Time/Question:  {metrics['avg_time']:.2f}s\")\n",
    "    print(f\"  - Retrieval:          {metrics['avg_retrieval_time']:.2f}s\")\n",
    "    print(f\"  - Generation:         {metrics['avg_generation_time']:.2f}s\")\n",
    "    print(\"=\" * 80)\n",
    "    \n",
    "    return metrics\n",
    "\n",
    "# Run performance test\n",
    "performance_test_questions = [\n",
    "    \"What is AWS Bedrock?\",\n",
    "    \"How does RAG work?\",\n",
    "    \"What are vector embeddings?\",\n",
    "    \"Explain best practices for RAG\",\n",
    "    \"What are common use cases for Bedrock?\"\n",
    "]\n",
    "\n",
    "metrics = evaluate_rag_performance(performance_test_questions, k=3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "1c4d0942-27b8-4264-9263-4da10c462cb1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
      "CONGRATULATIONS! RAG SYSTEM COMPLETE\n",
      "ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰ğŸ‰\n",
      "\n",
      "âœ… What You've Built:\n",
      "   - Document loading and chunking system\n",
      "   - Vector embedding generation with AWS Bedrock Titan\n",
      "   - FAISS vector database for efficient similarity search\n",
      "   - Claude LLM integration for answer generation\n",
      "   - Complete RAG pipeline with source attribution\n",
      "   - Save/load functionality for persistence\n",
      "   - Performance evaluation tools\n",
      "\n",
      "ğŸ“Š System Capabilities:\n",
      "   - Semantic search over your documents\n",
      "   - Context-aware question answering\n",
      "   - Source attribution for transparency\n",
      "   - Batch processing for multiple queries\n",
      "   - Interactive chat mode\n",
      "   - Performance metrics tracking\n",
      "\n",
      "ğŸš€ Next Steps:\n",
      "   1. Add more documents to your knowledge base\n",
      "   2. Experiment with different chunk sizes (300-1000 chars)\n",
      "   3. Try different values of k (number of retrieved chunks)\n",
      "   4. Adjust temperature in Claude for different creativity levels\n",
      "   5. Build a web interface with Streamlit or Gradio\n",
      "   6. Deploy as an API using FastAPI or Flask\n",
      "   7. Integrate with AWS Lambda for serverless deployment\n",
      "   8. Add conversation memory for multi-turn dialogue\n",
      "   9. Implement user feedback collection\n",
      "   10. Create custom evaluation metrics\n",
      "\n",
      "ğŸ’¡ Tips for Production:\n",
      "   - Implement proper error handling and logging\n",
      "   - Add rate limiting for Bedrock API calls\n",
      "   - Cache frequent queries to reduce costs\n",
      "   - Monitor token usage and costs\n",
      "   - Implement user authentication\n",
      "   - Add content filtering and safety checks\n",
      "   - Use AWS CloudWatch for monitoring\n",
      "   - Set up automated testing\n",
      "\n",
      "ğŸ“š Resources:\n",
      "   - AWS Bedrock Documentation: https://docs.aws.amazon.com/bedrock/\n",
      "   - LangChain Docs: https://python.langchain.com/\n",
      "   - FAISS Documentation: https://github.com/facebookresearch/faiss\n",
      "\n",
      "ğŸ¯ Ready to experiment? Try modifying the code and see what happens!\n",
      "\n",
      "================================================================================\n",
      "Happy Building! ğŸš€\n",
      "================================================================================\n"
     ]
    }
   ],
   "source": [
    "# Cell 19: Final Summary and Next Steps\n",
    "\n",
    "print(\"\\n\" + \"ğŸ‰\" * 40)\n",
    "print(\"CONGRATULATIONS! RAG SYSTEM COMPLETE\")\n",
    "print(\"ğŸ‰\" * 40)\n",
    "\n",
    "print(\"\"\"\n",
    "âœ… What You've Built:\n",
    "   - Document loading and chunking system\n",
    "   - Vector embedding generation with AWS Bedrock Titan\n",
    "   - FAISS vector database for efficient similarity search\n",
    "   - Claude LLM integration for answer generation\n",
    "   - Complete RAG pipeline with source attribution\n",
    "   - Save/load functionality for persistence\n",
    "   - Performance evaluation tools\n",
    "\n",
    "ğŸ“Š System Capabilities:\n",
    "   - Semantic search over your documents\n",
    "   - Context-aware question answering\n",
    "   - Source attribution for transparency\n",
    "   - Batch processing for multiple queries\n",
    "   - Interactive chat mode\n",
    "   - Performance metrics tracking\n",
    "\n",
    "ğŸš€ Next Steps:\n",
    "   1. Add more documents to your knowledge base\n",
    "   2. Experiment with different chunk sizes (300-1000 chars)\n",
    "   3. Try different values of k (number of retrieved chunks)\n",
    "   4. Adjust temperature in Claude for different creativity levels\n",
    "   5. Build a web interface with Streamlit or Gradio\n",
    "   6. Deploy as an API using FastAPI or Flask\n",
    "   7. Integrate with AWS Lambda for serverless deployment\n",
    "   8. Add conversation memory for multi-turn dialogue\n",
    "   9. Implement user feedback collection\n",
    "   10. Create custom evaluation metrics\n",
    "\n",
    "ğŸ’¡ Tips for Production:\n",
    "   - Implement proper error handling and logging\n",
    "   - Add rate limiting for Bedrock API calls\n",
    "   - Cache frequent queries to reduce costs\n",
    "   - Monitor token usage and costs\n",
    "   - Implement user authentication\n",
    "   - Add content filtering and safety checks\n",
    "   - Use AWS CloudWatch for monitoring\n",
    "   - Set up automated testing\n",
    "\n",
    "ğŸ“š Resources:\n",
    "   - AWS Bedrock Documentation: https://docs.aws.amazon.com/bedrock/\n",
    "   - LangChain Docs: https://python.langchain.com/\n",
    "   - FAISS Documentation: https://github.com/facebookresearch/faiss\n",
    "\n",
    "ğŸ¯ Ready to experiment? Try modifying the code and see what happens!\n",
    "\"\"\")\n",
    "\n",
    "print(\"=\" * 80)\n",
    "print(\"Happy Building! ğŸš€\")\n",
    "print(\"=\" * 80)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8852bb13-513b-4114-9bb2-8e6efc7eead4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: gradio in /opt/conda/lib/python3.12/site-packages (6.3.0)\n",
      "Requirement already satisfied: aiofiles<25.0,>=22.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (24.1.0)\n",
      "Requirement already satisfied: anyio<5.0,>=3.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (4.12.0)\n",
      "Requirement already satisfied: brotli>=1.1.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (1.1.0)\n",
      "Requirement already satisfied: fastapi<1.0,>=0.115.2 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.124.4)\n",
      "Requirement already satisfied: ffmpy in /opt/conda/lib/python3.12/site-packages (from gradio) (1.0.0)\n",
      "Requirement already satisfied: gradio-client==2.0.3 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.0.3)\n",
      "Requirement already satisfied: groovy~=0.1 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.1.2)\n",
      "Requirement already satisfied: httpx<1.0,>=0.24.1 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.28.1)\n",
      "Requirement already satisfied: huggingface-hub<2.0,>=0.33.5 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.36.0)\n",
      "Requirement already satisfied: jinja2<4.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.1.6)\n",
      "Requirement already satisfied: markupsafe<4.0,>=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.0.3)\n",
      "Requirement already satisfied: numpy<3.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (1.26.4)\n",
      "Requirement already satisfied: orjson~=3.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (3.11.4)\n",
      "Requirement already satisfied: packaging in /opt/conda/lib/python3.12/site-packages (from gradio) (23.2)\n",
      "Requirement already satisfied: pandas<3.0,>=1.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.3.3)\n",
      "Requirement already satisfied: pillow<13.0,>=8.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (11.3.0)\n",
      "Requirement already satisfied: pydantic<=3.0,>=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.12.5)\n",
      "Requirement already satisfied: pydub in /opt/conda/lib/python3.12/site-packages (from gradio) (0.25.1)\n",
      "Requirement already satisfied: python-multipart>=0.0.18 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.0.20)\n",
      "Requirement already satisfied: pyyaml<7.0,>=5.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (6.0.3)\n",
      "Requirement already satisfied: safehttpx<0.2.0,>=0.1.7 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.1.7)\n",
      "Requirement already satisfied: semantic-version~=2.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (2.10.0)\n",
      "Requirement already satisfied: starlette<1.0,>=0.40.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.50.0)\n",
      "Requirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.13.3)\n",
      "Requirement already satisfied: typer<1.0,>=0.12 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.20.0)\n",
      "Requirement already satisfied: typing-extensions~=4.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (4.15.0)\n",
      "Requirement already satisfied: uvicorn>=0.14.0 in /opt/conda/lib/python3.12/site-packages (from gradio) (0.38.0)\n",
      "Requirement already satisfied: fsspec in /opt/conda/lib/python3.12/site-packages (from gradio-client==2.0.3->gradio) (2024.12.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/conda/lib/python3.12/site-packages (from anyio<5.0,>=3.0->gradio) (3.11)\n",
      "Requirement already satisfied: annotated-doc>=0.0.2 in /opt/conda/lib/python3.12/site-packages (from fastapi<1.0,>=0.115.2->gradio) (0.0.4)\n",
      "Requirement already satisfied: certifi in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (2025.11.12)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/conda/lib/python3.12/site-packages (from httpx<1.0,>=0.24.1->gradio) (1.0.9)\n",
      "Requirement already satisfied: h11>=0.16 in /opt/conda/lib/python3.12/site-packages (from httpcore==1.*->httpx<1.0,>=0.24.1->gradio) (0.16.0)\n",
      "Requirement already satisfied: filelock in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (3.20.0)\n",
      "Requirement already satisfied: hf-xet<2.0.0,>=1.1.3 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (1.2.1)\n",
      "Requirement already satisfied: requests in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (2.32.5)\n",
      "Requirement already satisfied: tqdm>=4.42.1 in /opt/conda/lib/python3.12/site-packages (from huggingface-hub<2.0,>=0.33.5->gradio) (4.67.1)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2024.2)\n",
      "Requirement already satisfied: tzdata>=2022.7 in /opt/conda/lib/python3.12/site-packages (from pandas<3.0,>=1.0->gradio) (2025.3)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/conda/lib/python3.12/site-packages (from pydantic<=3.0,>=2.0->gradio) (0.7.0)\n",
      "Requirement already satisfied: pydantic-core==2.41.5 in /opt/conda/lib/python3.12/site-packages (from pydantic<=3.0,>=2.0->gradio) (2.41.5)\n",
      "Requirement already satisfied: typing-inspection>=0.4.2 in /opt/conda/lib/python3.12/site-packages (from pydantic<=3.0,>=2.0->gradio) (0.4.2)\n",
      "Requirement already satisfied: click>=8.0.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (8.3.1)\n",
      "Requirement already satisfied: shellingham>=1.3.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\n",
      "Requirement already satisfied: rich>=10.11.0 in /opt/conda/lib/python3.12/site-packages (from typer<1.0,>=0.12->gradio) (14.2.0)\n",
      "Requirement already satisfied: six>=1.5 in /opt/conda/lib/python3.12/site-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\n",
      "Requirement already satisfied: markdown-it-py>=2.2.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (4.0.0)\n",
      "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /opt/conda/lib/python3.12/site-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\n",
      "Requirement already satisfied: mdurl~=0.1 in /opt/conda/lib/python3.12/site-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n",
      "Requirement already satisfied: charset_normalizer<4,>=2 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (3.4.4)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in /opt/conda/lib/python3.12/site-packages (from requests->huggingface-hub<2.0,>=0.33.5->gradio) (1.26.20)\n",
      "* Running on local URL:  http://127.0.0.1:7860\n",
      "* Running on public URL: https://2fa97ff0f66814918d.gradio.live\n",
      "\n",
      "This share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div><iframe src=\"https://2fa97ff0f66814918d.gradio.live\" width=\"100%\" height=\"500\" allow=\"autoplay; camera; microphone; clipboard-read; clipboard-write;\" frameborder=\"0\" allowfullscreen></iframe></div>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": []
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "ğŸ” Searching for: 'how does RAG works?'\n",
      "ğŸ“Š Retrieving top 3 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0071, Distance: 140.1044)\n",
      "   Text preview: **How RAG Works**:\n",
      "RAG combines the power of retrieval-based techniques and generative language mode...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0070, Distance: 140.9529)\n",
      "   Text preview: **Why Use RAG?**\n",
      "1. **Combining Retrieval and Generation**:\n",
      "   - RAG combines the **precision of inf...\n",
      "\n",
      "ğŸ“Œ Result 3 (Score: 0.0070, Distance: 142.8384)\n",
      "   Text preview: **Applications of RAG**:\n",
      "1. **Customer Support Systems**:\n",
      "   - RAG-powered chatbots can pull relevan...\n",
      "\n",
      "\n",
      "ğŸ” Searching for: 'How does RAG work?'\n",
      "ğŸ“Š Retrieving top 3 results...\n",
      "\n",
      "ğŸ“Œ Result 1 (Score: 0.0076, Distance: 130.3964)\n",
      "   Text preview: **Why Use RAG?**\n",
      "1. **Combining Retrieval and Generation**:\n",
      "   - RAG combines the **precision of inf...\n",
      "\n",
      "ğŸ“Œ Result 2 (Score: 0.0073, Distance: 136.4141)\n",
      "   Text preview: **Applications of RAG**:\n",
      "1. **Customer Support Systems**:\n",
      "   - RAG-powered chatbots can pull relevan...\n",
      "\n",
      "ğŸ“Œ Result 3 (Score: 0.0072, Distance: 137.1158)\n",
      "   Text preview: **How RAG Works**:\n",
      "RAG combines the power of retrieval-based techniques and generative language mode...\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# In SageMaker notebook\n",
    "!pip install gradio\n",
    "\n",
    "import gradio as gr\n",
    "\n",
    "def rag_interface(question, num_sources):\n",
    "    \"\"\"Gradio interface function\"\"\"\n",
    "    results = search_similar_chunks(question, k=int(num_sources))\n",
    "    context = [r['text'] for r in results]\n",
    "    answer = generate_answer(question, context)\n",
    "    \n",
    "    sources_text = \"\\n\\n\".join([\n",
    "        f\"[{i+1}] Relevance: {r['similarity_score']:.2%}\\n{r['text'][:200]}...\"\n",
    "        for i, r in enumerate(results)\n",
    "    ])\n",
    "    \n",
    "    return answer, sources_text\n",
    "\n",
    "# Create interface\n",
    "demo = gr.Interface(\n",
    "    fn=rag_interface,\n",
    "    inputs=[\n",
    "        gr.Textbox(label=\"Your Question\", placeholder=\"Ask anything...\"),\n",
    "        gr.Slider(1, 5, value=3, step=1, label=\"Number of Sources\")\n",
    "    ],\n",
    "    outputs=[\n",
    "        gr.Textbox(label=\"Answer\", lines=10),\n",
    "        gr.Textbox(label=\"Sources\", lines=10)\n",
    "    ],\n",
    "    title=\"ğŸ¤– RAG Q&A System\",\n",
    "    description=\"Ask questions about your documents!\",\n",
    "    theme=\"soft\"\n",
    ")\n",
    "\n",
    "# Launch with public link\n",
    "demo.launch(share=True)  # Creates a public URL!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f20fe8-f8f2-4538-8698-65f8a6b8fe46",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
